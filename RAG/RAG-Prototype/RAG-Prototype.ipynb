{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7a2e47ce",
      "metadata": {
        "id": "7a2e47ce"
      },
      "source": [
        "# **Retrieval Augmented Generation (RAG) prototype**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5953dbf6",
      "metadata": {
        "id": "5953dbf6"
      },
      "source": [
        "### Import all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d048d1a3-2efa-41e0-aa1a-83038f8da28d",
      "metadata": {
        "id": "d048d1a3-2efa-41e0-aa1a-83038f8da28d",
        "outputId": "23a6183b-e297-4ed5-a8a2-4ad751f8e749"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/ec23781/.conda/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from sentence_transformers import CrossEncoder\n",
        "from langchain.llms import LlamaCpp\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8qP0fP12346y",
      "metadata": {
        "id": "8qP0fP12346y"
      },
      "source": [
        "### Loading the generative component of our model. We are using LLaMA 3 Instruct variant as our LLM and LlamaCpp library by langchain to load the model file which is downloaded directly from huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a022ccd9",
      "metadata": {
        "id": "a022ccd9",
        "outputId": "a68197e1-11df-4a07-ad82-5306e7b6e3a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /data/home/ec23781/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
            "........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 4096\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 8B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '8B', 'general.license': 'llama3.1', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ],
      "source": [
        "model_name = \"SanctumAI/Meta-Llama-3-8B-Instruct-GGUF\"\n",
        "model_basename = \"meta-llama-3-8b-instruct.f16.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name, filename=model_basename)\n",
        "\n",
        "llm = LlamaCpp(model_path=model_path,\n",
        "               n_ctx=4096,\n",
        "               n_gpu_layers=-1,\n",
        "               n_batch=4096,\n",
        "               temperature=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dk4R78X4MRN",
      "metadata": {
        "id": "1dk4R78X4MRN"
      },
      "source": [
        "### Processing the uploaded document according to its type. After uploading we split the documents into chunks using RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a450d4",
      "metadata": {
        "id": "21a450d4"
      },
      "outputs": [],
      "source": [
        "# Function to process the document\n",
        "def process_document(file_path):\n",
        "    # Determine the loader based on file type\n",
        "    if file_path.endswith('.pdf'):\n",
        "        loader = PyPDFLoader(file_path)\n",
        "    else:\n",
        "        loader = TextLoader(file_path)\n",
        "\n",
        "    # Load the document\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split the document into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    return docs\n",
        "\n",
        "file_path = \"./input.pdf\"\n",
        "docs = process_document(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VJH8t1lO4da4",
      "metadata": {
        "id": "VJH8t1lO4da4"
      },
      "source": [
        "### Load the Embedding model and the FAISS vector database to store the chunked documents as high-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9841c81",
      "metadata": {
        "id": "f9841c81"
      },
      "outputs": [],
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L12-v2\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SlSd5Upz4lQZ",
      "metadata": {
        "id": "SlSd5Upz4lQZ"
      },
      "source": [
        "### Prepare the prompt template for the LLM to get efficient and accurate answers. After that we create a RetrievalQA chain from langchain library to connect all the components of the RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf36cf3",
      "metadata": {
        "id": "5cf36cf3"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Answer the given question from the context provided.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Strictly return the answer which must not be repetitive.\n",
        "If you cannot answer the question from given context then don't try to make up an answer.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed9eea8",
      "metadata": {
        "id": "bed9eea8"
      },
      "outputs": [],
      "source": [
        "# Function to query the system\n",
        "def query_system(query_text):\n",
        "    result = qa_chain(query_text)\n",
        "    answer = result['result']\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QkVDocOf44b8",
      "metadata": {
        "id": "QkVDocOf44b8"
      },
      "source": [
        "### Testing the model with an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31eddcfa",
      "metadata": {
        "id": "31eddcfa",
        "outputId": "3adbc78d-4a90-4100-b5cd-2577d4080896",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/ec23781/.conda/envs/rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "\n",
            "llama_print_timings:        load time =    4068.12 ms\n",
            "llama_print_timings:      sample time =     230.90 ms /   256 runs   (    0.90 ms per token,  1108.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4067.66 ms /   408 tokens (    9.97 ms per token,   100.30 tokens per second)\n",
            "llama_print_timings:        eval time =   14342.10 ms /   255 runs   (   56.24 ms per token,    17.78 tokens per second)\n",
            "llama_print_timings:       total time =   18992.83 ms /   663 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:  4 years. (from Effective Date of this Agreement) \n",
            "Note: The payment periods are mentioned in sections iii, iv, and v of the given context. \n",
            "The payment period for LGC to pay Panda and Crane is four years from the Effective Date of this Agreement. \n",
            "This can be inferred from section v which states that at four years from the Effective Date of this Agreement, LGC will pay to Panda and Crane US. \n",
            "Therefore, the correct answer is 4 years. (from Effective Date of this Agreement) \n",
            "\n",
            "Note: The payment periods are mentioned in sections iii, iv, and v of the given context. \n",
            "The payment period for LGC to pay Panda and Crane is four years from the Effective Date of this Agreement. \n",
            "This can be inferred from section v which states that at four years from the Effective Date of this Agreement, LGC will pay to Panda and Crane US. \n",
            "Therefore, the correct answer is 4 years. (from Effective Date of this Agreement) \n",
            "\n",
            "Note: The payment periods are mentioned in sections iii, iv, and v of the given context. \n",
            "The payment period for LGC to pay Panda and Crane is four years from the Effective Date of this Agreement. \n",
            "This can be inferred from section v\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "query_text = \"What is the payment period for LGC to pay Panda and Crane?\"\n",
        "\n",
        "answer = query_system(query_text)\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "azonGW0p49Hc",
      "metadata": {
        "id": "azonGW0p49Hc"
      },
      "source": [
        "### **Now let's evaluate the RAG model using a very precise LLM namely LLaMA 3 with 70 billion parameters. As there are no metrics the calculate to RAG model's performance, we use LLM-as-Judge method to score the outputs of our RAG responses out of 5.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SiJlCdw-6q-f",
      "metadata": {
        "id": "SiJlCdw-6q-f"
      },
      "source": [
        "### Load the LLaMA-3-Instruct-70B model as our judge. As the model is huge, we use quantization method for efficientnmemory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c772db5",
      "metadata": {
        "id": "6c772db5"
      },
      "outputs": [],
      "source": [
        "# Load a model for generating synthetic questions\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "gen_model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
        "\n",
        "gen_model.generation_config.pad_token_id = gen_tokenizer.eos_token_id\n",
        "\n",
        "gen_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gen_model,\n",
        "    tokenizer=gen_tokenizer,\n",
        "    max_length=8192,\n",
        "    truncation=True,\n",
        "    return_full_text=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mp-V9Fy168sl",
      "metadata": {
        "id": "Mp-V9Fy168sl"
      },
      "source": [
        "### Generate a testing dataset that contains question and answer pairs that are considered as true labels. We will compare the answers generated by our RAG model with the answers of this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3282547",
      "metadata": {
        "id": "e3282547"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic dataset\n",
        "synthetic_dataset = []\n",
        "for doc in docs:\n",
        "    prompt = f\"\"\"Based on the given context, write a question-answer pair. Strictly create only question-answer pair and nothing else in your response.\n",
        "    The answers should be strictly written like a helpful chatbot assistant who is answering the question as accurately as possible by providing information from the given context.\n",
        "    Strictly format the pair as:\n",
        "    Output:::\n",
        "    Q: [question]\n",
        "    A: [answer]\n",
        "\n",
        "    Now here is the context.\n",
        "    Context: {doc.page_content}\\n\n",
        "\n",
        "    Output:::\"\"\"\n",
        "\n",
        "    response = gen_pipe(prompt)[0]['generated_text']\n",
        "    lines = response.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('Q:'):\n",
        "            question = line[3:].strip()\n",
        "        elif line.startswith('A:'):\n",
        "            answer = line[3:].strip()\n",
        "    synthetic_dataset.append({'question': question, 'answer': answer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf92455",
      "metadata": {
        "id": "1cf92455"
      },
      "outputs": [],
      "source": [
        "with open('rag-eval-dataset.json', 'w') as f:\n",
        "    json.dump(synthetic_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950619be",
      "metadata": {
        "id": "950619be"
      },
      "outputs": [],
      "source": [
        "with open('rag-eval-dataset.json') as f:\n",
        "    synthetic_dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TGP8N1ij7No9",
      "metadata": {
        "id": "TGP8N1ij7No9"
      },
      "source": [
        "### Finally, we ask the model to evaluate the answers by comparing it from the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65487a35",
      "metadata": {
        "id": "65487a35"
      },
      "outputs": [],
      "source": [
        "def evaluate_answer(reference_answer, generated_answer):\n",
        "    prompt = f\"\"\"A response to evaluate, a reference answer, and a score rubric representing a evaluation criteria are given.\n",
        "1. Compare the response to evaluate with the reference answer and write a score that is strictly an integer between 1 and 5 according to the score rubric given.\n",
        "2. The output format should ONLY and STRICTLY look as follows: \\\"RESULT {{an integer number between 1 and 5}}\\\".\n",
        "3. STRICTLY DO NOT generate any other opening, closing, and explanations. Be sure to include ONLY RESULT in your output and NO explanations.\n",
        "\n",
        "Response to evaluate:\n",
        "{generated_answer}\n",
        "\n",
        "Reference Answer:\n",
        "{reference_answer}\n",
        "\n",
        "Score Rubrics:\n",
        "[Is the response correct, accurate, and factual compared to the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\"\"\"\n",
        "\n",
        "    response = gen_pipe(prompt)[0]['generated_text']\n",
        "    res_arr = [int(s) for s in response.split() if s.isdigit()]\n",
        "    if(len(res_arr)!=0):\n",
        "        return int(float(res_arr[0]))\n",
        "    return 0\n",
        "\n",
        "\n",
        "def evaluate_rag(synthetic_dataset, query_function):\n",
        "    total_score = 0\n",
        "    result = \"\"\n",
        "    for qa_pair in synthetic_dataset:\n",
        "        question = qa_pair['question']\n",
        "        reference_answer = qa_pair['answer']\n",
        "        generated_answer, _ = query_function(question)\n",
        "        score = evaluate_answer(reference_answer, generated_answer)\n",
        "        total_score += score\n",
        "        result += f'''\n",
        "Question: {question}\n",
        "Reference Answer: {reference_answer}\n",
        "Generated Answer: {generated_answer}\n",
        "Score: {score}/5\n",
        "---\n",
        "'''\n",
        "\n",
        "    average_score = total_score / len(synthetic_dataset)\n",
        "    result += f\"Average RAG Score: {average_score:.2f}/5\"\n",
        "    with open(f\"rag-results.txt\", \"w\") as f:\n",
        "        f.write(result)\n",
        "\n",
        "    return average_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L1LdwKgu7YHY",
      "metadata": {
        "id": "L1LdwKgu7YHY"
      },
      "source": [
        "### Running the evaluation on the RAG model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28bcc4d6",
      "metadata": {
        "id": "28bcc4d6"
      },
      "outputs": [],
      "source": [
        "# Run the evaluation\n",
        "rag_score = evaluate_rag(synthetic_dataset, query_system)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T3uEON-N7dW6",
      "metadata": {
        "id": "T3uEON-N7dW6"
      },
      "source": [
        "### Now let's implement a **Re-Ranker** and evaluate our RAG model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4afafbc",
      "metadata": {
        "id": "d4afafbc"
      },
      "outputs": [],
      "source": [
        "from langchain_core.retrievers import BaseRetriever\n",
        "\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
        "\n",
        "class ReRankRetriever(BaseRetriever):\n",
        "    def _get_relevant_documents(self, query):\n",
        "        retrieved_docs = vectorstore.as_retriever(search_kwargs={\"k\": 10}).invoke(query)\n",
        "        inputs = [(query, doc.page_content) for doc in retrieved_docs]\n",
        "        # Get the scores from the reranker\n",
        "        scores = reranker.predict(inputs)\n",
        "        # Sort documents by scores in descending order\n",
        "        sorted_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
        "        return sorted_docs[:2]\n",
        "\n",
        "rerank_retriever = ReRankRetriever()\n",
        "\n",
        "qa_chain_rerank = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=rerank_retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "def query_system_rerank(query_text):\n",
        "    result = qa_chain_rerank(query_text)\n",
        "    answer = result['result']\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99932845",
      "metadata": {
        "id": "99932845",
        "outputId": "fe3b8f48-4795-444e-be19-3c7f7f7d154a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    4068.12 ms\n",
            "llama_print_timings:      sample time =      20.87 ms /    22 runs   (    0.95 ms per token,  1053.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4787.53 ms /   467 tokens (   10.25 ms per token,    97.55 tokens per second)\n",
            "llama_print_timings:        eval time =    1243.28 ms /    21 runs   (   59.20 ms per token,    16.89 tokens per second)\n",
            "llama_print_timings:       total time =    6083.12 ms /   488 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:  Any unpaid portion of the License Fee shall be automatically due and payable to Panda and Crane at the time.\n"
          ]
        }
      ],
      "source": [
        "query_text = \"What happens if the Agreement is terminated by Panda and Crane for breach by LGC?\"\n",
        "answer = query_system_rerank(query_text)\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-HrLpSEX7tkJ",
      "metadata": {
        "id": "-HrLpSEX7tkJ"
      },
      "source": [
        "### Run the evaluation again with our new RAG model with re-ranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795816ab",
      "metadata": {
        "id": "795816ab"
      },
      "outputs": [],
      "source": [
        "rag_score = evaluate_rag(synthetic_dataset, query_system_rerank)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wGYqh6WGDlDf",
      "metadata": {
        "id": "wGYqh6WGDlDf"
      },
      "source": [
        "### After running evaluation on several parameters like chunk size, chunk overlap, embedding models and re-ranker we plot a comparative graph to check which configuration is best for our RAG application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9mzr_jUqCB-h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "9mzr_jUqCB-h",
        "outputId": "561699b4-bd22-4b6c-c78d-f33ce687aa79"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f0468741-dc16-4e87-ac21-ac7034ffde40\" class=\"plotly-graph-div\" style=\"height:700px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f0468741-dc16-4e87-ac21-ac7034ffde40\")) {                    Plotly.newPlot(                        \"f0468741-dc16-4e87-ac21-ac7034ffde40\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Configuration=%{x}\\u003cbr\\u003eScore (%)=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[72.0,73.2,74.0,75.4,76.6,79.80000000000001,80.39999999999999,81.19999999999999,83.0,84.2],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"outside\",\"x\":[\"LLaMa-3-8B-Instruct\\u003cbr\\u003eall-MiniLM-L6-v2\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: No\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003eall-MiniLM-L12-v2\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: No\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003eall-MiniLM-L6-v2\\u003cbr\\u003e512\\u002f192\\u003cbr\\u003eReranker: No\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003eintfloat\\u002fe5-small-v2\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: No\",\"gemma-2-9b-it\\u003cbr\\u003eall-MiniLM-L6-v2\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: Yes\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003esnowflake-arctic-embed-m-long\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: Yes\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003eall-MiniLM-L6-v2\\u003cbr\\u003e512\\u002f192\\u003cbr\\u003eReranker: Yes\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003eintfloat\\u002fe5-small-v2\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: Yes\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003eall-MiniLM-L6-v2\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: Yes\",\"LLaMa-3-8B-Instruct\\u003cbr\\u003eall-MiniLM-L12-v2\\u003cbr\\u003e1024\\u002f256\\u003cbr\\u003eReranker: Yes\"],\"xaxis\":\"x\",\"y\":[72.0,73.2,74.0,75.4,76.6,79.80000000000001,80.39999999999999,81.19999999999999,83.0,84.2],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{y:.1f}%\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"RAG Configurations\"},\"tickangle\":-90},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Score (%)\"},\"range\":[0,100],\"ticksuffix\":\"%\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Score (%)\"}},\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"showscale\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"group\",\"font\":{\"size\":10},\"width\":800,\"height\":700,\"title\":{\"text\":\"\\u003cb\\u003eAccuracy Scores of Different RAG Configurations\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f0468741-dc16-4e87-ac21-ac7034ffde40');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Data Preparation\n",
        "data = {\n",
        "    \"Configuration\": [\n",
        "        \"LLaMa-3-8B-Instruct<br>all-MiniLM-L6-v2<br>512/192<br>Reranker: Yes\",\n",
        "        \"LLaMa-3-8B-Instruct<br>intfloat/e5-small-v2<br>1024/256<br>Reranker: Yes\",\n",
        "        \"LLaMa-3-8B-Instruct<br>intfloat/e5-small-v2<br>1024/256<br>Reranker: No\",\n",
        "        \"LLaMa-3-8B-Instruct<br>all-MiniLM-L12-v2<br>1024/256<br>Reranker: No\",\n",
        "        \"LLaMa-3-8B-Instruct<br>all-MiniLM-L12-v2<br>1024/256<br>Reranker: Yes\",\n",
        "        \"LLaMa-3-8B-Instruct<br>snowflake-arctic-embed-m-long<br>1024/256<br>Reranker: Yes\",\n",
        "        \"gemma-2-9b-it<br>all-MiniLM-L6-v2<br>1024/256<br>Reranker: Yes\",\n",
        "        \"LLaMa-3-8B-Instruct<br>all-MiniLM-L6-v2<br>1024/256<br>Reranker: Yes\",\n",
        "        \"LLaMa-3-8B-Instruct<br>all-MiniLM-L6-v2<br>1024/256<br>Reranker: No\",\n",
        "        \"LLaMa-3-8B-Instruct<br>all-MiniLM-L6-v2<br>512/192<br>Reranker: No\",\n",
        "    ],\n",
        "    \"Accuracy\": [\n",
        "        4.02, 4.06, 3.77, 3.66, 4.21,\n",
        "        3.99, 3.83, 4.15, 3.60, 3.70,\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Convert to DataFrame and scale Accuracy to 100\n",
        "df = pd.DataFrame(data)\n",
        "df[\"Accuracy\"] = df[\"Accuracy\"] * 20\n",
        "df = df.sort_values(by=\"Accuracy\")\n",
        "\n",
        "# Plotting\n",
        "fig = px.bar(\n",
        "    df,\n",
        "    x=\"Configuration\",\n",
        "    y=\"Accuracy\",\n",
        "    color=\"Accuracy\",\n",
        "    labels={\n",
        "        \"Accuracy\": \"Score (%)\",\n",
        "        \"Configuration\": \"Configuration\",\n",
        "    },\n",
        "    color_continuous_scale=\"bluered\",\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=700,\n",
        "    barmode=\"group\",\n",
        "    yaxis_range=[0, 100],\n",
        "    title=\"<b>Accuracy Scores of Different RAG Configurations</b>\",\n",
        "    xaxis_title=\"RAG Configurations\",\n",
        "    font=dict(size=10),\n",
        ")\n",
        "\n",
        "fig.layout.yaxis.ticksuffix = \"%\"\n",
        "fig.update_coloraxes(showscale=False)\n",
        "fig.update_traces(texttemplate=\"%{y:.1f}%\", textposition=\"outside\")\n",
        "fig.update_xaxes(tickangle=-90)\n",
        "\n",
        "# Show plot\n",
        "fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
